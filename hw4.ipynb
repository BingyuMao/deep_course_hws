{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TW-ri0ae1l1Y"
   },
   "source": [
    "### Import packages and get access to the training text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOsPipPkq4r3",
    "outputId": "70c72695-38d1-4feb-fc42-b3063bee4b3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Colab Notebooks/6334/HW4\n"
     ]
    }
   ],
   "source": [
    "### For colab usage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd drive/My Drive/Colab Notebooks/6334/HW4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVqj46rcPfVD",
    "outputId": "f0b53f50-141c-4030-e833-6fa4e3c9d861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█▍                              | 10 kB 19.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 20 kB 23.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 30 kB 12.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 40 kB 9.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 51 kB 5.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 61 kB 5.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 71 kB 5.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 81 kB 6.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 92 kB 4.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 102 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 112 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 122 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 133 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 143 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 153 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 163 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 174 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 184 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 194 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 204 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 215 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 225 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 235 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 235 kB 5.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWg80xxQycC9"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vYppyYIpvQG"
   },
   "outputs": [],
   "source": [
    "### helpers.py\n",
    "\n",
    "def read_file(filename):\n",
    "    file = unidecode.unidecode(open(filename).read())\n",
    "    return file, len(file)\n",
    "\n",
    "filename = 'medline.0.txt'\n",
    "file, file_len = read_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7eklqsO1xv9"
   },
   "source": [
    "### Functions to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V-y0OCQygyF"
   },
   "outputs": [],
   "source": [
    "### train.py\n",
    "### corrected one mistake in train of using cuda\n",
    "### corrected one mistake in the last line of train: previous code is loss.data[0] \n",
    "### which causes error, so changed to loss\n",
    "\n",
    "def random_training_set(chunk_len, batch_size):\n",
    "    inp = torch.LongTensor(batch_size, chunk_len)\n",
    "    target = torch.LongTensor(batch_size, chunk_len)\n",
    "    for bi in range(batch_size):\n",
    "        start_index = random.randint(0, file_len - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = file[start_index:end_index]\n",
    "        inp[bi] = char_tensor(chunk[:-1])\n",
    "        target[bi] = char_tensor(chunk[1:])\n",
    "    inp = Variable(inp)\n",
    "    target = Variable(target)\n",
    "    if cuda:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    return inp, target\n",
    "\n",
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "\n",
    "    if cuda: \n",
    "        #Can't convert hidden to cuda because hidden is a tuple of tensor, not tensor. \n",
    "        #Need to convert it to list, then convert each of the elements to cuda,\n",
    "        #then convert back to a tuple.\n",
    "        hidden = list(hidden)\n",
    "        hidden[0] = hidden[0].cuda()\n",
    "        hidden[1] = hidden[1].cuda()\n",
    "        hidden = tuple(hidden)\n",
    "        #hidden = hidden.cuda()\n",
    "\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[:,c], hidden)\n",
    "        loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss / chunk_len\n",
    "\n",
    "def save():\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "    print('Saved as %s' % save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwO9Zxdjyg3v"
   },
   "outputs": [],
   "source": [
    "### model.py\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "            #self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, bias=False) #try to set bias as False\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "cWW3322ZPfVJ"
   },
   "source": [
    "### Another model with dropout layer\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1, drop_prob=0.05):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.model = model.lower()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        if self.model == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        elif self.model == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        output = self.dropout(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.model == \"lstm\":\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UpdqrjLy99W"
   },
   "outputs": [],
   "source": [
    "### generate.py\n",
    "### corrected one mistake of using cuda, the same as in train\n",
    "\n",
    "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=False):\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n",
    "\n",
    "    if cuda:\n",
    "        #Can't convert hidden to cuda because hidden is a tuple of tensor, not tensor. \n",
    "        #Need to convert it to list, then convert each of the elements to cuda,\n",
    "        #then convert back to a tuple.\n",
    "        hidden = list(hidden)\n",
    "        hidden[0] = hidden[0].cuda()\n",
    "        hidden[1] = hidden[1].cuda()\n",
    "        hidden = tuple(hidden)\n",
    "        #hidden = hidden.cuda()\n",
    "        prime_input = prime_input.cuda()\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "        \n",
    "    inp = prime_input[:,-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
    "        if cuda:\n",
    "            inp = inp.cuda()\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYW6jisB2lQw"
   },
   "outputs": [],
   "source": [
    "### healpers.py\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = all_characters.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laoPc1XR28hD"
   },
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkX3AKM13bbg"
   },
   "outputs": [],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "n_layers = 2\n",
    "n_epochs = 2000\n",
    "\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "model = \"lstm\"\n",
    "\n",
    "chunk_len = 500\n",
    "print_every = 100\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8bQrTjjyg6P"
   },
   "outputs": [],
   "source": [
    "decoder = CharRNN(\n",
    "    n_characters,\n",
    "    hidden_size,\n",
    "    n_characters,\n",
    "    model=model,\n",
    "    n_layers=n_layers,\n",
    ")\n",
    "\n",
    "if cuda:\n",
    "    decoder.cuda()\n",
    "\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdzYuQ45yg_Q",
    "outputId": "d12c0cb1-13a9-404a-fa63-080f7a4b2c35",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 100/2000 [02:54<57:06,  1.80s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2m 54s (100 5%) 1.7931]\n",
      "Whanas ang the and in tumury weras in thor. Herkerk (TM) apsine recermes (\n",
      "SB  - BACF (TQA The in scum \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 200/2000 [05:46<51:53,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5m 46s (200 10%) 1.4480]\n",
      "Whytics\n",
      "MH  - Adult\n",
      "MH  - Ordags\n",
      "MH  - Contite Skuakizations D\n",
      "AU  - Brainei A\n",
      "FAU - Recang, F\n",
      "AU  - L \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 300/2000 [08:43<49:31,  1.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8m 43s (300 15%) 1.2067]\n",
      "Whuan purtaliate benics and anoticle\n",
      "      study in the poprotective \n",
      "      immunomogressy group prost \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [11:37<47:50,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11m 37s (400 20%) 1.1709]\n",
      "Whit, Son KC\n",
      "FAU - Valgerferraha, Krano\n",
      "AU  - El M\n",
      "FAU - Traki, Micha\n",
      "AU  - Sentotwiel J\n",
      "FAU - Mida Ba \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 500/2000 [14:29<42:35,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14m 29s (500 25%) 1.1298]\n",
      "Wheliation compared to the been strement and and PGC/Ried to\n",
      "      patients with by and the supported  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 600/2000 [17:24<39:35,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17m 24s (600 30%) 1.1121]\n",
      "Whin (c) 2012 Elsevier Sequence. In HCD4-IIF-2.07%, and 40 was multiver decreased in the progrative/me \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 700/2000 [20:16<37:50,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20m 16s (700 35%) 1.0367]\n",
      "Whed Resign, Thow-Flts a symphological such Society OMCA protein, human cancer is evaluate the\n",
      "      t \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 800/2000 [23:08<35:25,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23m 8s (800 40%) 1.0687]\n",
      "Whreary, Young Neoplasms/chemistry/*metabolism\n",
      "MH  - *Sensity, Department of Urology\n",
      "MH  - Netherlands \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 900/2000 [26:06<36:13,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26m 6s (900 45%) 1.0298]\n",
      "Whuie and\n",
      "      and localized system\n",
      "      of the NIPC chemitherapy\n",
      "OT  - ended to complecteria of the \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1000/2000 [29:01<30:03,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29m 1s (1000 50%) 1.0538]\n",
      "Whes sembard cancer and breast carcinomase Dadds (191; Phy 3). The patients\n",
      "      provides assessed an \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1100/2000 [32:05<27:12,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32m 5s (1100 55%) 1.0394]\n",
      "Whard Fundestic Acids/*therapeutic use\n",
      "MH  - Enzymial Protein/analysis/*adverse effects/metabolism/pat \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1200/2000 [34:59<23:21,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34m 59s (1200 60%) 0.9627]\n",
      "Wh protein, hulm a lings. In\n",
      "      will four surgical of more levels, in bion and regulated in the eas \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1300/2000 [37:53<20:11,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37m 53s (1300 65%) 0.9973]\n",
      "Wh/TOR JUTOMET- NODS: Taignal Pharmaceutical Cell\n",
      "      Yenosis, Patterning\n",
      "MH  - Polyphrom Histone Re \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1400/2000 [40:49<17:07,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40m 49s (1400 70%) 1.0118]\n",
      "Wh prochemistancy. UL/United States and inactiductic cancer. RESULTS: Aptory of metastases. Patients w \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1500/2000 [43:44<14:26,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43m 44s (1500 75%) 1.0189]\n",
      "Wh\n",
      "PS  - 1R428-1425 (Print)\n",
      "IS  - 0022-4514 (Linking)\n",
      "VI  - 33\n",
      "IP  - 6\n",
      "DP  - 2012 Dact 10.88-2662-6000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1600/2000 [46:39<11:43,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46m 39s (1600 80%) 0.9519]\n",
      "Whi and cisplatin and intervention of orkers. Herein cells of the decreased in 27 were beas ablable at \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1700/2000 [49:43<08:44,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49m 43s (1700 85%) 0.9318]\n",
      "Wheaus Cancer-Cells, Colon(2)(5) to rectal on the HSVs in woming\n",
      "      participate also information an \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1800/2000 [52:38<05:54,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52m 38s (1800 90%) 0.9610]\n",
      "Whe directs analysis in vistem. METHOD: Bacterial oxidative in some Kapea \n",
      "      OS or self-care in ga \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1900/2000 [55:33<02:56,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55m 33s (1900 95%) 0.9823]\n",
      "Wha indicates for negative statistically sample. The improve approaches of the however and medical \n",
      "   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [58:25<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58m 25s (2000 100%) 1.0069]\n",
      "Whar C\n",
      "FIR - Cattogo, Andrie\n",
      "IR  - Pertad G\n",
      "FIR - Roserenst, Rean\n",
      "IR  - Park A\n",
      "FIR - Delberg, Taka\n",
      "AU  \n",
      "\n",
      "Saving...\n",
      "Saved as medline.0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "    loss = train(*random_training_set(chunk_len, batch_size))\n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(decoder, 'Wh', 100, cuda=cuda), '\\n')\n",
    "\n",
    "print(\"Saving...\")\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1YNYqdcyhEA",
    "outputId": "c561207a-da5d-4a23-9c8d-94a5c185da85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PMID- 23017638\\nOWN - NLM\\nSTAT- MEDLINE\\nDCOM- 20130313\\nLR  - 20171125\\nIS  - 1568-7142 (Electronic)\\nIS  - 0009-4963 (Linking)\\nVI  - 143\\nIP  - 1\\nDP  - 2013 Feb\\nTI  - The simulate of\\n      limited the simulating number of the progression and positive\\n      not assays, demethy, its receptor response to the increase of CAT, use of the mice.\\nPG  - 697-647 CPHD-100254241005 [pii]\\nAID - 10.1158/1078-01007-1-123. Epub 2012 Sep 27.\\n\\nPMID- 22989364\\nOWN - NLM\\nSTAT- MEDLINE\\nDCOM- 20130521\\nLR  - 20171108\\nIS  - 1539-3627 (Electronic)\\nIS  - 0742-1865 (Linking)\\nVI  - 18\\nIP  - 11\\nDP  - 2012 Dec\\nTI  - Cancer carbold who various cell lines in Biol directly that\\n      someter for helpyly cell cancer and 10 healthy show use of RATC3 delivery of\\n      the cisplatin-casurgery therapy of study, we should be the diagnosis\\n      interactions, and has \\n      with cancer health targeting the pathways and\\n      performed to level from large discus symptom in human PSA (3041) and we to tumor proposed potentially and important in versus cell lines and the expression in \\n      and weights origipocytes that the prostate spacer groups and\\n      development of ER-alphodipin, which the control genome (Z-dos)\\nRN  - EC 2.7.1.- oxurgery\\n      C and LCS, 198 and CA men and method\\n      there mouse the tu-diment was pathway disease.\\nCI  - Copyright (c) 2012 Elsevidle N Corticular Gene\\n      USA. Clinical, Nata Mauzia.\\nFAU - Dini, Mariach\\nAU  - Surgen R\\nAD  - Department of Medicinal Acadecinks\\nEDAT- 2012/09/27 06:00\\nMHDA- 2013/03/23 06:00\\nCRDT- 2012/09/26 06:00\\nPHST- 2012/09/29 06:00 [entrez]\\nPHST- 2012/09/26 06:00 [pubmed]\\nPHST- 2013/06/22 06:00 [medline]\\nAID - 10.1096/MET013-95559 [doi]\\nPST - ppublish\\nSO  - Arch Estly Lhap Proteins/genetics\\nMH  - Disease Progression Regulation)\\nRN  - 0 ytoline Dis Netherlands\\nAU  - Jingulasa\\nIR  - Weksken DLLC\\nFIR - Fang-Chaun, Yi Y\\nIR  - Zhardi M\\nFAU - Colepelan, Lrao\\nAU  - Mutzexotiti A\\nFAU - Holan, Mariane\\nAU  - Andrer G\\nFAU - Konsso, Swabi\\nAU  - Bolleya G\\nFAU - Li, Shulia\\nA'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0VymNRnyhMq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_BingyuMao.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
